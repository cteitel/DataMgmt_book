# Data Cleaning in R {#qaqc}

## Objectives

* Fix issues related to reading data and data formatting
* Summarize and visualize data to identify likely errors
* Resolve missing data using tools in R

## Principles of data cleaning

Many of the tools we will use to clean data in R are functions and approaches we have already covered. Learning how to use these approaches to identify, fix, and document errors in data is a key part of a reproducible workflow. Although you *could* fix many of these issues manually in Excel, doing so programatically provides a record of your methods and changes. It also allows you to use the same approaches over and over again, with less effort each time. I use these approaches a lot because I often use data collected by other people, so it's important that I have a clear record of what changes I have made, so that I can check back with the data collectors to see if these make sense.

## Fixing issues with data entry and reading data

Some errors in data come in through inconsistencies in data entry, or incompatibility of data entry with the way R reads data. One issue that you can encounter before even getting data into R deals with special characters (i.e., symbols). Some of these have to be ["encoded,"](https://kunststube.net/encoding/) meaning that they are replaced by a code rather than read in as the symbol itself. For the most part, this is for symbols that aren't on your keyboard, like Greek letters and (in this case) the degree sign. 

```{r, error = T}
library(tidyverse)
fish <- read.csv("data/raw/messy_georgia_fish_data.csv") 
```

We can change the encoding that R expects with an additional argument:

```{r, error = T}
fish <- read.csv("data/raw/messy_georgia_fish_data.csv", fileEncoding = "latin1") 
```

In this case, I usually find it easiest to play around with the encoding options until something works - it's more work to figure out the encoding in the original file, and that information is sometimes unavailable.

### Column names

R has a few rules for column names: they cannot contain spaces or symbols and cannot start with numbers. If you read in a data file with these characteristics, R will do its best to fix them - but not usually in the way you would like. In this case `read_csv` and `read.csv` act differently: `read_csv` creates a tibble, which can handle spaces, symbols, and numbers as long as they are referenced in tick marks (` `), but `read.csv` will rename your columns.

```{r, error = T}
fish_base <- read.csv("data/raw/messy_georgia_fish_data.csv", fileEncoding = "latin1") 
names(fish_base)

fish_in <- read_csv("data/raw/messy_georgia_fish_data.csv")
names(fish_in)
```

```{r, error = T, eval = F, echo = T}
head(fish_in$1st Method Used) #returns an error
```

```{r}
head(fish_in$`1st Method Used`)
```

*Side note: Notice that I did not need to include the encoding in `read_csv()`. This is because it's a little more likely than `read.csv()` to figure out the encoding by default.*

Although `read_csv` works a little better here, it's still annoying to access columns with ticks, and they won't work in all functions. The `rename()` function in `dplyr` becomes useful here. The `rename_with()` function allows us to rename multiple columns using a rule:

```{r}
fish <- rename(fish_in, Method1 = `1st Method Used`,
               Method2 = `2nd Method Used`)
names(fish)
fish <- rename_with(fish, ~str_replace_all(.x, " ", "_"))
fish <- rename_with(fish, ~str_remove_all(.x, "[)]|[(]"))
names(fish)
```

The syntax of the `rename_with()` function introduces a couple of new ways of working with `tidyverse` functions. Often, `summarize()`, `rename()`, and so on expect a function as their second argument. The function is preceded by a tilde (~) and using `.x` as an argument tells R to use column names.

Also note that I renamed my data from `fish_in` to `fish`. This can help you if you want to compare your clean and raw data later.

We might also want to follow general best practices and get rid of capital letters:

```{r}
fish <- rename_with(fish, str_to_lower)
names(fish)
```

This is another way of using these functions. If they don't require any other arguments, we can just include the function name (without a ~). The code above is the same as:

```{r}
fish <- rename_with(fish, ~str_to_lower(.x))
```

### Multi-format columns

Sometimes, data are entered in such a way that R cannot tell the appropriate format. In this case, you will most often end up with a character column because almost anything can be converted to a character without losing information. We can check the format of the columns using `str()` or `glimpse()` from tidyverse.

```{r}
glimpse(fish)
```

It looks like we have problems with two columns: **date_collected** should be a date and *coordinates* should really be two columns (latitude and longitude). Just looking at the first few elements of date_collected, it looks like it's all in a month/day/year format, so let's try that:

```{r}
fish <- mutate(fish, date = mdy(date_collected))
sum(is.na(fish$date))
```

The warning tells us that 25 of the elements were not in month/day/year format. Let's look at that in more detail:

```{r}
unique(fish$date_collected)
```

It looks like we have a few formats. Luckily, `lubridate` treats names of months as the same as their numbers, so we don't have to worry about that. Here is where `if_else()` comes in. Since `mdy()` returns `NA` if it can't parse a date, we can take advantage of that:

```{r}
fish <- mutate(fish, date = mdy(date_collected),
               date = if_else(is.na(date), dmy(date_collected), date),
               date = if_else(is.na(date), ymd(date_collected), date))
sum(is.na(fish$date))
```

The logic here is that we first use one method to parse dates. If that doesn't work, we use another - but we only apply this to dates where the first method returned `NA`, and we source the date to parse from the original date column. We still get a lot of warnings because we have to go through the `NA`s. We also still have five `NA`s, but looking at those more closely, they all had missing dates in the original data:

```{r}
filter(fish, is.na(date)) %>%
  select(date_collected)
```

### Strings and special characters

The coordinates column highlights another issue: while R read in the special character (degree symbol) eventually, it doesn't look right and doesn't provide us with information here; we don't usually include units within our data. 

```{r}
head(fish$coordinates)
```

The coordinates also appear to be in a combination of degrees-minutes-seconds format rather than in decimal degrees. Here, we need to split latitude and longitude, split apart the components, and then use them to calculate decimal degrees:

```{r}
fish <- fish %>%
  mutate(latitude = str_split_fixed(coordinates, pattern = ",", n = 2)[,1],
         longitude = str_split_fixed(coordinates, ",", 2)[,2])
select(fish, latitude, longitude) %>% head()
```

```{r}
latitudes <- fish$latitude %>%
  #split by degree symbol and minutes symbol
  str_split_fixed(pattern = "\xa1|'|\"", n = 3) %>%
  #convert from matrix to tibble and rename
  as_tibble() %>%
  setNames(c("deg","min","sec"))
latitudes <- latitudes %>%
  #remove north designation and seconds symbol
  mutate(across(everything(), ~str_remove_all(.x, "N|\""))) %>%
  #convert strings to numeric
  mutate(across(everything(), as.numeric)) %>%
  #convert to decimal degrees
  mutate(dd = deg + min/60 + sec/(60*60))

longitudes <- str_split_fixed(fish$longitude, pattern = "\xa1|'|\"", n = 3) %>%
  as_tibble() %>%
  setNames(c("deg","min","sec"))
longitudes <- mutate(longitudes, across(everything(), ~str_remove_all(.x, "W|\""))) %>%
  mutate(across(everything(), as.numeric)) %>%
  mutate(dd = deg + min/60 + sec/(60*60))

fish <- bind_cols(fish, lat = latitudes$dd, long = longitudes$dd)
head(fish %>% select(coordinates, lat, long))

```

That was a *lot* of steps. There's probably a package out there that would do it for you, but if you know the basics it can be faster to use them than to figure out the syntax of new functions.

## Identifying likely errors

Now that the data are formatted correctly, we still need to make sure the data points themselves are valid. Two main flavors of incorrect data are duplicates and incorrect entries (sometimes called outliers).

### Duplicates

A duplicate is just that: a piece of data that has been entered twice. *True duplicates* are data points that share all the same information. They make our lives easy because they are the same - we just need to identify them and remove one. Other duplicates are more ambiguous; for example, I sometimes come across GPS locations from a single animal that have the same timestamp with different coordinates. In this case, we can look at the values to see which is more likely (see "outliers" below) or randomly choose one.

To check for duplicates, we can use `group_by()` and `summarize()`:

```{r}
fish %>%
  group_by(across(everything())) %>% #use all columns as groups
  summarize(n = n()) %>% #count the number in each group
  filter(n > 1) #filter to groups with more than one entry
```

Here, we know that the data are identical, so we can just take the first of these rows:

```{r}
fish <- fish %>%
  group_by(across(everything())) %>% #use all columns as groups
  slice(1) #take the first item in each group
nrow(fish_in) - nrow(fish)
```

I have removed one row.

If there are any other columns that should be unique identifiers, I can use them here. For example, I think there should only be one row per sample ID.

```{r}
fish %>%
  group_by(sample_id) %>% #use only sample ID as group
  summarize(n = n()) %>% #count the number in each group
  filter(n > 1) #filter to groups with more than one entry
```

Let's look at that sample:

```{r}
filter(fish, sample_id == 21)
```

We can look at which columns differ:

```{r}
filter(fish, sample_id == 21) %>%
  group_by(sample_id) %>% #group by sample ID
  summarize(across(everything(), n_distinct)) %>% #get the number of unique values in each column
  pivot_longer(cols = -sample_id) #pivot_longer just turns this into columns for visualization
```

It looks like method1 is our problem:

```{r}
filter(fish, sample_id == 21) %>%
  pull(method1)
```

We have a capitalization problem! While we're at it, why don't we make sure that is fixed for all character columns:

```{r}
fish <- fish %>%
  mutate(common_name = str_to_title(common_name),
         scientific_name = str_to_sentence(scientific_name),
         sample_location = str_to_title(sample_location),
         method1 = str_to_lower(method1),
         method2 = str_to_lower(method2))
```

Now we can remove that final duplicate:

```{r}
fish <- fish %>%
  group_by(sample_id) %>% #use all columns as groups
  slice(1) %>% #take the first item in each group
  ungroup()
nrow(fish_in) - nrow(fish)
```

### Outliers

For quantitative variables, outliers are values outside the expected range of the data. Some outliers are true data - for example, an organism that is very large or very small for its species - but some are due to errors in data entry, instrumentation, or sampling. For example, a juvenile might have been measured in a sample that was supposed to only be adults. To identify outliers, we can look at the range and distributions of our numeric variables. In the fish dataset, we only have one numeric variable: fish length.

```{r}
range(fish$fish_length_mm, na.rm=T)
hist(fish$fish_length_mm)
```

It looks like we have a bunch of lengths that are way too long to be reasonable. Let's look at those values:

```{r}
fish %>%
  filter(fish_length_mm > 6000) %>%
  pull(fish_length_mm) %>%
  unique()
```

We see there are two outliers here: 9,999 and 10,000. 9999 is a common code for missing data, so we probably want to replace those with NAs. 10,000 is probably an error in data entry. Let's convert those all to NAs:

```{r}
fish <- fish %>%
  mutate(fish_length_mm = if_else(fish_length_mm > 6000, NA, fish_length_mm))
hist(fish$fish_length_mm)
```

That looks better.

Sometimes these errors can be corrected. For example, if we were sure that the 10000 was supposed to be entered as 100.00, we could replace that value:

```{r}
fish <- fish %>%
  mutate(fish_length_mm = if_else(fish_length_mm == 10000, 100, fish_length_mm))
```

In non-numeric data, the equivalent of an outlier is an unexpected value. The easiest way to look at this can be by listing the unique values of a vector/column:

```{r}
unique(fish$sample_location)
```

Here, we probably want to replace "?" with `NA`.

```{r}
fish <- fish %>%
  mutate(sample_location = if_else(sample_location == "?", NA, sample_location))
```

## Dealing with missing data

Most ecological data sets contain some missing data. There are three main ways to deal with this.

### Filtering

The most obvious way to deal with missing data is to ignore data points that are missing. When doing so, a key question to ask yourself is which missing data matter. For example, if we are not using spatial data, missing coordinates will not matter. The `filter()` function combined with `if_any()` or `if_all()` is your friend here. For example, we might only want to filter data if length is missing:

```{r}
fish_len_complete <- filter(fish, !is.na(fish_length_mm))
```

Or if all the location columns are complete:

```{r}
fish_loc_complete <- filter(fish, if_all(c(sample_location, long, lat), ~!is.na(.x)))
# Equivalent to:
fish_loc_complete <- filter(fish, !if_any(c(sample_location, long, lat), ~is.na(.x)))
```

The code above can be a little confusing. In the first option, we are asking for the case in which *all* of the location columns are *not NA*. In the second option, we are asking for the case in which it is *not* true that *any* of hte location columns are NA. Take a close look and see if you can figure out the logic. Sometimes this kind of backwards thinking is necessary for filtering.

An easier, but less streamlined alternative is:

```{r}
fish_loc_complete <- filter(fish, !is.na(sample_location) &
                              !is.na(long) &
                              !is.na(lat))
```

### Imputing

Like in our example of `fish_length_mm == 10000` above, you might know the reason for some missing values, allowing you to impute them (i.e., artificially add them to the dataset). You might also be able to use other data in the dataset to figure it out. For example, here we have some missing coordinates where we have location names:

```{r}
filter(fish, is.na(long) & !is.na(sample_location)) %>%
  select(sample_location, coordinates, lat, long)
```

But since we have coordinates for that location name elsewhere, we can impute it. First, we should make sure this location name has only one lat/long associated:

```{r}
crnra_coords <- fish %>%
  filter(sample_location == "Chattahoochee@Crnra" &
           !is.na(long)) %>%
  distinct(long, lat)
nrow(crnra_coords)
```

```{r}
fish <- fish %>%
  mutate(lat = if_else(sample_location == "Chattahoochee@Crnra" & is.na(lat), crnra_coords$lat, lat),
         long = if_else(sample_location == "Chattahoochee@Crnra" & is.na(long), crnra_coords$long, long))
```

Depending on why the data are missing and how they will be used, other methods for imputing data include:

* Predictions from a model
* Averages from the rest of the data set
* Proxies based on other measurements

## Saving clean data

Finally, after our data is clean, we save a new version of it in the "clean" data folder.

```{r}
write_csv(fish, "data/clean/georgia_fish_data_clean.csv")
```
